%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University/School Laboratory Report
% LaTeX Template
% Version 3.1 (25/3/14)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% Linux and Unix Users Group at Virginia Tech Wiki 
% (https://vtluug.org/wiki/Example_LaTeX_chem_lab_report)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass{article}

%\usepackage[version=3]{mhchem} % Package for chemical equation typesetting
%\usepackage{siunitx} % Provides the \SI{}{} and \si{} command for typesetting SI units
\usepackage{graphicx} % Required for the inclusion of images
%\usepackage{natbib} % Required to change bibliography style to APA
\usepackage{amsmath} % Required for some math elements 
\usepackage{float}
\usepackage{changepage}



\setlength\parindent{0pt} % Removes all indentation from paragraphs

%\renewcommand{\labelenumi}{\alph{enumi}.} % Make numbering in the enumerate environment by letter rather than number (e.g. section 6)

%\usepackage{times} % Uncomment to use the Times New Roman font

%----------------------------------------------------------------------------------------
%	DOCUMENT INFORMATION
%----------------------------------------------------------------------------------------

\title{Fundaments of HPC \\ Second Assignment} % Title

\author{Nicola \textsc{Domenis}} % Author name

\date{\today} % Date for the report

\begin{document}

\maketitle % Insert the title, author and date

\begin{center}
\begin{tabular}{l r}

\end{tabular}
\end{center}

% If you wish to include an abstract, uncomment the lines below
% \begin{abstract}
% Abstract text
% \end{abstract}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Introduction}

We present the second assignment in the course of FHPC. We will discuss about:


% If you have more than one objective, uncomment the below:
\begin{description}
\item[Exercise Zero] \hfill \\
Objective 1 text
\item[Exercise One] \hfill \\
Objective 2 text
\end{description} 
 
%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Exercise 0}

We start by observing two slightly different codes: 01\_array\_sum.c
and 04\_touch\_by\_all.c . They implement the same algorithm: summing the first N natural integers. They both use the OpenMP standard to take on a parallel approach with multiple threads.
The analysis of the programs can start by a strong scalability test.

\subsection(Strong Scalability Test)
We report the elapsed times versus the number of cores and also the speedups versus the number of cores:

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/exercise_0_strongscaling} % Example image
	\caption{Elapsed times for $N=10^9$}
\end{figure}

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/exercise_0_speedup} % Example image
	\caption{Speedup for $N=10^9$}
\end{figure}


We see that the code scales for the same size of the problem and that the code that implements the "`touch by all"' policy is faster.

\subsection(Parallel Overhead)
Then we can see how to calculate the parallel overhead. We can use the formula $ e(n,p) =\frac{\frac{1}{S_p(n,t)}-\frac{1}{p}}{1-\frac{1}{p}}$ to estimate the parallel overhead. A plot with the measures we have would return:

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/exercise_0_overhead} % Example image
	\caption{Overhead for $N=10^9$}
\end{figure}
The touch\_by\_all code once again is better than the array\_sum code, since it has a lower parallel overhead.
But both lines are almost constant, showing that both codes have an almost constant parallel overhead while we increment the number of processors.

\subsection(Performance evaluation)
Now lets compare some valuable metrics for the two codes' executions:

\begin{adjustwidth}{-3cm}{}
\begin{tabular}[H]{l|l |l l l l| l l}
Metric & measures&&&&&mean&\\
CPU cycles& array\_sum &372889698& 143797361&671828181&373809554&390581199&-\\
          & touch\_by\_all&602487792&16412868&15030736&15901403&162458200&= \\
&&&&&&&	228122999\\				
					\hline
Cache misses & array\_sum&9459& 11492&49514&9484&19987.25&-\\
          & touch\_by\_all& 10356&10256 & 8417&12081&10277.5&=\\
&&&&&&&	 9709.75\\				
\hline
Elapsed time   & array\_sum& 0,014486573&0,007287606&0,024476034& 0,014344770&0.01514875&- \\
          & touch\_by\_all &0,021872540&0,002893806&0,002676134&0,002677902&0.007530096&=\\
					&&&&&&&	 0.00761865\\				
\hline
\end{tabular}
\end{adjustwidth}
Even if the single performances, measured with Perf, are quite similar in their randomness, we see that the touch\_by\_all code outperforms in average the array\_sum code in all the metrics.
%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------

\section{Exercise 1}

We need to rewrite the code mpi\_pi.c by means of OpenMP directives. We had to solve all the problems we encountered in lesson, like avoiding race conditions and false sharing.
In the end we produced the simple code openmp\_pi.c.
\subsection(Strong and Weak Scalability)
We show the plots for the strong and weak scalability:


\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/openmp_pi_strongscaling} % Example image
	\caption{Strong scaling test for $N=10^9$}
\end{figure}

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/openmp_pi_strongscaling_speedup} % Example image
	\caption{Speedup for $N=10^9$}
\end{figure}

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/openmp_pi_weakscaling} % Example image
	\caption{Weak Scaling test for $N=10^4*P$}
\end{figure}

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/openmp_pi_weakscaling_speedup} % Example image
	\caption{Weak Scaling speedup for $N=10^4*P$}
\end{figure}
In the strong scalability setting the code scales very well, the speedup is almost linear. But the weak scalability test shows that the program doesn't handle well problems of increasing size through an increase of threads.
\subsection(Parallel overhead)

Now we estimate the parallel overhead, using again the formula to calulate $e(n,p)$ when we have an increasing number of threads on multiple cores:


\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/openmp_pi_strongscaling_overhead} % Example image
	\caption{Overhead for $N=10^4*P$}
\end{figure}
We see that the overhead estimation remains almost constant, showing that the program's parallel overhead doesn't increase over the number of processors.
\subsection(Comparison with mpi\_pi.c)
Now it's time to test the performance of mpi\_pi.c on a single node and on multiple nodes. We started with 1 node with 20 cores and we finished with 10 cores having 2 nodes each. We see if splitting processors on multiple nodes gives a slower execution time due to the network.

\begin{figure}[H] % [h] forces the figure to be output where it is defined in the code (it suppresses floating)
	\centering
	\includegraphics[width=0.8\columnwidth]{graphs/mpi_pi_elaps_time_on_nodes} % Example image
	\caption{Overhead for $N=10^4*P$}
\end{figure}

We see that the separation among nodes slightly slows down the execution time. The openmp_pi.c program running on 20 threads would instead have an elapsed time of 1.56 seconds, which is faster. The inter-core communication is faster than inter-node communication. Latency and maybe overhead
%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------
%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------



%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------


%----------------------------------------------------------------------------------------
%	BIBLIOGRAPHY
%----------------------------------------------------------------------------------------
%---------------------------------------------------------------------------------------


\end{document}