mi loggo nel cluster Ulysses:
cecilia@cecilia-E402SA:~/Desktop/FHPC_2019-2020_(mio)/D03/code$ ssh -l cbertolo frontend2.hpc.sissa.it
Warning: Permanently added the RSA host key for IP address '147.122.11.33' to the list of known hosts.
cbertolo@frontend2.hpc.sissa.it's password: 
Last login: Sun Oct 20 16:02:09 2019 from 151.68.27.53
                                                              
Queue system documentation: please see   man queues

FAIR USAGE: this is a login node, not a computing node. Long-running jobs 
            should be queued, not run here, and will be automatically killed
            when they exceed their memory or cputime limit.



2018-12-14   STORAGE FAILURE

             one of the two storage controllers that power shared filesystems
             (/home and /scratch) is DOWN
             
             all your data are safe and accessible, but you will probably 
             experience a performance degradation while reading and writing
             to /home and /scratch (depending on overall access pattern
             expected access times could be 2 to 10 times longer than usual)
             
             
[cbertolo@login2 ~]$ module avail

--------------------------------------------------------- /u/shared/modules/compilers ---------------------------------------------------------
bazel/0.4.5/gnu/4.9.2    cudatoolkit/6.5(default) gcc/4.8.2                intel/18.4               pgi_CE/16.10-openmpi
cudatoolkit/10.0         cudatoolkit/7.5          gnu/4.9.2(default)       pgi/14.6                 pgi_CE/17.4(default)
cudatoolkit/6.0          cudatoolkit/9.2          intel/14.0               pgi_CE/16.10             pgi_CE/17.4-openmpi

------------------------------------------------------------ /u/shared/modules/mpi ------------------------------------------------------------
impi-trial/5.0.1.035             mvapich2/2.1a/intel/14.0         openmpi/1.8.3/intel/14.0
mvapich2/2.1a/gnu/4.9.2          openmpi/1.8.3/gnu/4.9.2(default)

--------------------------------------------------------- /u/shared/modules/libraries ---------------------------------------------------------
2decomp_fft/1.5/fftw/3.3.4/intel/14.0                     matplotlib/1.4.3/python/2.7.8/numpy/1.9.1
atlas/3.10.2/gnu/4.9.2                                    matplotlib/1.4.3/python/3.4.2/numpy/1.9.1
boost/1.57.0/gnu/4.9.2/python/2.7.8                       mkl/11.1
fftw/2.1.5/gnu/4.9.2                                      netcdf/4.3.2/gnu/4.9.2
fftw/2.1.5/intel/14.0                                     netcdf-cxx/4.2/gnu/4.9.2
fftw/3.3.4/gnu/4.9.2                                      netcdf-fortran/4.4.1/gnu/4.9.2
fftw/3.3.4/intel/14.0                                     numpy/1.9.1/intel/14.0/mkl/11.1/python/2.7.8
fftw/3.3.4/old(default)                                   numpy/1.9.1/intel/14.0/mkl/11.1/python/3.4.2
fftw/3.3.4-single/gnu/4.9.2                               openblas/0.2.13/gnu/4.9.2
fftw/3.3.4-single/intel/14.0                              plasma/1.6.1/intel/14.0/mkl/11.1
gsl/1.16/gnu/4.9.2                                        scipy/0.15.1/numpy/1.9.1/intel/14.0/mkl/11.1/python/2.7.8
gsl/2.2/gnu/4.9.2                                         scipy/0.15.1/numpy/1.9.1/intel/14.0/mkl/11.1/python/3.4.2
hdf5/1.8.12/gnu/4.9.2                                     tensorflow/1.1.0/gnu/4.9.2/cpu
magma/1.6.1/intel/14.0/mkl/11.1/cudatoolkit/6.5           tensorflow/1.1.0/gnu/4.9.2/gpu

------------------------------------------------------- /u/shared/modules/applications --------------------------------------------------------
cmake/2.8.12(default)  cmake/3.1.1            cplex/12.6.3-community idl/8.4.1              python/2.7.8/gnu/4.9.2 python/3.4.2/gnu/4.9.2

--------------------------------------------------------- /u/shared/modules/utilities ---------------------------------------------------------
SMR3102                contribs               gnuplot/5.0.0          hwloc/1.10.0(default)  likwid/3.1.3/gnu/4.9.2 numactl/2.0.10
cluster-utils          gnuplot/4.6.6(default) gnuplot/5.0.6          hwloc/1.11.0           likwid/4.0.0/gnu/4.9.2 testing

[cbertolo@login2 ~]$ nano
	--> i create a file with nano , copy and paste the script , save 'mpi_pi.c' 

[cbertolo@login2 ~]$ ls
mpi_pi.c

[cbertolo@login2 ~]$ mpicc mpi_pi.c -o mpi_pi.x
-bash: mpicc: command not found

	-> because we have to load module openmpi

[cbertolo@login2 ~]$ module load openmpi/1.8.3/gnu/4.9.2
loading dependency gnu/4.9.2
loading dependency numactl/2.0.10
loading dependency hwloc/1.10.0

[cbertolo@login2 ~]$ mpicc mpi_pi.c -o mpi_pi.x

[cbertolo@login2 ~]$ ldd mpi_pi.x
	linux-vdso.so.1 =>  (0x00007ffec9fb5000)
	libmpi.so.1 => /u/shared/programs/x86_64/openmpi/1.8.3/gnu/4.9.2/torque/lib/libmpi.so.1 (0x00007f379c1de000)
	libpthread.so.0 => /lib64/libpthread.so.0 (0x0000003555c00000)
	libc.so.6 => /lib64/libc.so.6 (0x0000003555400000)
	libopen-rte.so.7 => /u/shared/programs/x86_64//openmpi/1.8.3/gnu/4.9.2/torque/lib/libopen-rte.so.7 (0x00007f379bf52000)
	libopen-pal.so.6 => /u/shared/programs/x86_64//openmpi/1.8.3/gnu/4.9.2/torque/lib/libopen-pal.so.6 (0x00007f379bca4000)
	libdl.so.2 => /lib64/libdl.so.2 (0x0000003555800000)
	librt.so.1 => /lib64/librt.so.1 (0x0000003556800000)
	libutil.so.1 => /lib64/libutil.so.1 (0x000000355b000000)
	libhwloc.so.5 => /u/shared/programs/x86_64//hwloc/1.10.0/lib/libhwloc.so.5 (0x00007f379ba62000)
	libm.so.6 => /lib64/libm.so.6 (0x0000003556000000)
	libnuma.so.1 => /u/shared/programs/x86_64/numactl/2.0.10/lib/libnuma.so.1 (0x00007f379b856000)
	libltdl.so.7 => /usr/lib64/libltdl.so.7 (0x0000003556c00000)
	/lib64/ld-linux-x86-64.so.2 (0x0000003555000000)

[cbertolo@login2 ~]$ ./mpi_pi.x 
--------------------------------------------------------------------------
Error obtaining unique transport key from ORTE (orte_precondition_transports not present in
the environment).

  Local host: login2
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[login2:30297] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!


[cbertolo@login2 ~]$ ./mpi_pi.x 1000
--------------------------------------------------------------------------
Error obtaining unique transport key from ORTE (orte_precondition_transports not present in
the environment).

  Local host: login2
--------------------------------------------------------------------------
--------------------------------------------------------------------------
It looks like MPI_INIT failed for some reason; your parallel process is
likely to abort.  There are many reasons that a parallel process can
fail during MPI_INIT; some of which are due to configuration or environment
problems.  This failure appears to be an internal failure; here's some
additional information (which may only be relevant to an Open MPI
developer):

  PML add procs failed
  --> Returned "Error" (-1) instead of "Success" (0)
--------------------------------------------------------------------------
*** An error occurred in MPI_Init
*** on a NULL communicator
*** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
***    and potentially your MPI job)
[login2:30418] Local abort before MPI_INIT completed successfully; not able to aggregate error messages, and not able to guarantee that all other processes were killed!
[cbertolo@login2 ~]$ git clone https://github.com/sissa/FHPC_2019-2020.git
Initialized empty Git repository in /home/cbertolo/FHPC_2019-2020/.git/
error:  while accessing https://github.com/sissa/FHPC_2019-2020.git/info/refs

fatal: HTTP request failed

	--> run erato, dovevo usare mpirun 

°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°°

RICOMINCIAMOOOOOO !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
creo una cartella nella quale copio il conenuto della cartella code


[cbertolo@login2 ~]$ mkdir FHPC

[cbertolo@login2 ~]$ cd FHPC/

[cbertolo@login2 FHPC]$ cp -r /tmp/code_for_all/ ~/code

[cbertolo@login2 FHPC]$ cd ~/code/

[cbertolo@login2 code]$ ls

mpi_pi.c  mpi_pi.x  pi.c

[cbertolo@login2 code]$ module load openmpi/1.8.3/gnu 

[cbertolo@login2 code]$ module list 
Currently Loaded Modulefiles:
  1) gnu/4.9.2                 2) numactl/2.0.10            3) hwloc/1.10.0              4) openmpi/1.8.3/gnu/4.9.2

[cbertolo@login2 code]$ mpicc mpi_pi.c -o mpi_pi.x

[cbertolo@login2 code]$ mpirun -np 4 ./mpi_pi.x				(<--- errore perchè non specifico il "number_of_iterations")
 Usage : mpi -np n ./mpi_pi.x number_of_iterations 
 Usage : mpi -np n ./mpi_pi.x number_of_iterations 
 Usage : mpi -np n ./mpi_pi.x number_of_iterations 
 Usage : mpi -np n ./mpi_pi.x number_of_iterations 
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpirun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[63052,1],2]
  Exit code:    255
--------------------------------------------------------------------------
[cbertolo@login2 code]$ mpirun -np 4 ./mpi_pi.x 100000000		(<---con 4 processors)

 # walltime on processor 2 : 0.64070201 

 # walltime on processor 1 : 0.64523196 

 # walltime on processor 3 : 0.64996910 

 # of trials = 100000000 , estimate of pi is 3.141382160 

 # walltime on master processor : 0.65099287 

[cbertolo@login2 code]$ mpirun -np 8 ./mpi_pi.x 100000000		(<-- con 8 processors)

 # of trials = 100000000 , estimate of pi is 3.141541320 

 # walltime on master processor : 0.41991305 

 # walltime on processor 1 : 0.41149807 

 # walltime on processor 2 : 0.40172100 

 # walltime on processor 3 : 0.41567898 

 # walltime on processor 4 : 0.40660286 

 # walltime on processor 5 : 0.39246798 

 # walltime on processor 6 : 0.38752913 

 # walltime on processor 7 : 0.39682794 


	-->order non ce'è, parallelo usa il primo precessore libero 

[cbertolo@login2 code]$ qsub -l nodes=1:ppn=20 -I			<-- ???????????????????????
qsub: waiting for job 3389693.master1 to start

^CDo you wish to terminate the job and exit (y|[n])? y
Job 3389693.master1 is being deleted



